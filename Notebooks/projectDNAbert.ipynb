{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"eFVtW8GsNVsP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735557792512,"user_tz":-60,"elapsed":28132,"user":{"displayName":"OUAFAE ELFAIDI","userId":"10715894589759469388"}},"outputId":"6db6fa75-eeb4-4e35-97ca-b0cc268c1e68"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import re\n","import csv\n","\n","def calculate_cag_repeats(sequence):\n","    \"\"\"Calculates the number of CAG repeats in a DNA sequence\"\"\"\n","\n","    repeats = re.findall(r'(CAG)', sequence)\n","    repeat_count = len(repeats)\n","    return repeat_count\n","\n","def label_sequence(repeat_count):\n","    \"\"\"Label the sequence based on the number of CAG repeats\"\"\"\n","    if repeat_count <= 35:\n","        return 'Normal'\n","    elif 36 <= repeat_count <= 39:\n","        return 'Intermediate'\n","    elif 40 <= repeat_count <= 55:\n","        return 'Reduced_Penetrance'\n","    else:\n","        return 'Full_Mutation'\n","\n","\n","with open('/content/drive/MyDrive/our_project/dataset/huntington_sequences_5000.txt', 'r') as file:\n","    sequences = [line.strip() for line in file]\n","\n","\n","formatted_data = []\n","for sequence in sequences:\n","    repeat_count = calculate_cag_repeats(sequence)\n","    label = label_sequence(repeat_count)\n","    formatted_data.append([sequence, repeat_count, label])\n","\n","\n","with open('/content/drive/MyDrive/our_project/dataset/labeled_DATA.csv', 'w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow([\"Sequence\", \"CAG_Repeats\", \"Label\"])  # Write the header\n","    writer.writerows(formatted_data)  # Write the data\n","\n","print(\"Data formatted and saved to new2_Huntington.csv\")"],"metadata":{"id":"xapS6ok0P1xe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732909352238,"user_tz":-60,"elapsed":1842,"user":{"displayName":"OUAFAE ELFAIDI","userId":"10715894589759469388"}},"outputId":"724f06f8-40fe-4cb1-beb5-c262a469ddeb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data formatted and saved to new2_Huntington.csv\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import os\n","\n","class DNASequenceDataset(Dataset):\n","    def __init__(self, sequences, labels, tokenizer, max_length=512):\n","        self.sequences = sequences\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.sequences)\n","\n","    def __getitem__(self, idx):\n","        sequence = self.sequences[idx]\n","        label = self.labels[idx]\n","\n","        # Convert sequence to k-mers (k=6 for DNABERT)\n","        kmers = [sequence[i:i+6] for i in range(len(sequence)-5)]\n","        kmer_sequence = \" \".join(kmers)\n","\n","        encoding = self.tokenizer(\n","            kmer_sequence,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","def load_dna_sequences(file_path):\n","    df = pd.read_csv(file_path)\n","    sequences = df.iloc[:, 0].values  # First column contains sequences\n","    labels = df.iloc[:, -1].values    # Last column contains labels\n","    return sequences, labels\n","\n","def train_dnabert_model(file_path, model_save_dir, num_epochs=10):\n","    # Create model save directory if it doesn't exist\n","    os.makedirs(model_save_dir, exist_ok=True)\n","\n","    # Set device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Load and prepare data\n","    sequences, labels = load_dna_sequences(file_path)\n","\n","    # Encode labels\n","    label_encoder = LabelEncoder()\n","    encoded_labels = label_encoder.fit_transform(labels)\n","\n","    # Split data\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        sequences,\n","        encoded_labels,\n","        test_size=0.2,\n","        random_state=42\n","    )\n","\n","    # Load DNABERT tokenizer and model\n","    tokenizer = BertTokenizer.from_pretrained('zhihan1996/DNA_bert_6')\n","    model = BertForSequenceClassification.from_pretrained(\n","        'zhihan1996/DNA_bert_6',\n","        num_labels=len(label_encoder.classes_)\n","    ).to(device)\n","\n","    # Create datasets\n","    train_dataset = DNASequenceDataset(X_train, y_train, tokenizer)\n","    test_dataset = DNASequenceDataset(X_test, y_test, tokenizer)\n","\n","    # Create dataloaders\n","    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=16)\n","\n","    # Initialize optimizer\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n","\n","    # Training loop\n","    best_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0\n","\n","        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n","            optimizer.zero_grad()\n","\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                labels=labels\n","            )\n","\n","            loss = outputs.loss\n","            total_loss += loss.item()\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","        # Evaluation\n","        model.eval()\n","        correct = 0\n","        total = 0\n","\n","        with torch.no_grad():\n","            for batch in test_loader:\n","                input_ids = batch['input_ids'].to(device)\n","                attention_mask = batch['attention_mask'].to(device)\n","                labels = batch['labels'].to(device)\n","\n","                outputs = model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask\n","                )\n","\n","                _, predicted = torch.max(outputs.logits, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        accuracy = 100 * correct / total\n","        print(f'Epoch {epoch+1}: Average Loss = {total_loss/len(train_loader):.4f}, Accuracy = {accuracy:.2f}%')\n","\n","        if accuracy > best_accuracy:\n","            best_accuracy = accuracy\n","\n","            torch.save(model.state_dict(), os.path.join(model_save_dir, 'dnabert_model.pt'))\n","            torch.save(label_encoder, os.path.join(model_save_dir, 'label_encoder.pkl'))\n","            tokenizer.save_pretrained(model_save_dir)\n","\n","    return model, tokenizer, label_encoder\n","\n","def load_model(model_save_dir, device):\n","\n","    label_encoder = torch.load(os.path.join(model_save_dir, 'label_encoder.pkl'))\n","\n","\n","    tokenizer = BertTokenizer.from_pretrained(model_save_dir)\n","\n","\n","    model = BertForSequenceClassification.from_pretrained(\n","        model_save_dir,\n","        num_labels=len(label_encoder.classes_)\n","    ).to(device)\n","\n","    return model, tokenizer, label_encoder\n","\n","def predict_mutation(model, tokenizer, label_encoder, sequence, device):\n","    # Convert sequence to k-mers\n","    kmers = [sequence[i:i+6] for i in range(len(sequence)-5)]\n","    kmer_sequence = \" \".join(kmers)\n","\n","    # Tokenize\n","    encoding = tokenizer(\n","        kmer_sequence,\n","        add_special_tokens=True,\n","        max_length=512,\n","        padding='max_length',\n","        truncation=True,\n","        return_tensors='pt'\n","    )\n","\n","    # Move to device\n","    input_ids = encoding['input_ids'].to(device)\n","    attention_mask = encoding['attention_mask'].to(device)\n","\n","    # Get prediction\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n","        predicted_class = torch.argmax(probabilities, dim=1)\n","\n","    # Convert prediction to label\n","    predicted_label = label_encoder.inverse_transform(predicted_class.cpu().numpy())[0]\n","    probabilities = probabilities.cpu().numpy()[0]\n","\n","    return {\n","        'predicted_label': predicted_label,\n","        'probabilities': dict(zip(label_encoder.classes_, probabilities))\n","    }"],"metadata":{"id":"k1b9Z47jTVrB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" # Prediction example:\n","sequence = \"\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')ATGCGCGTATCAGGCCAAGTTCATGCCCGGGGCAGAATTAACAGCAGCAGCAGCAGCAGCAGCAGCAAATTAAGCAGCAGCAGCAGCAGCAGCAGCAGCAGCAATTAAAGCAGCAGCAGCAGCAGCAGCAGCAGCAGAATTAACAGCAGCAGCAGCAAATTAAGCAGCAGCAGCAGCAGCAGAATTAAAATTAAAATTAAGTCTAAATTCTTTGACAAAGCGACTTTGTACATTTTTACTAGACGTAATGCGTGACCATTATTTATTATAGGCAACGCTTCACTGAAAGTCTAAAGGTTAACGGGTCTCGAGTTATCTTGTGTGCTGTATCCGGGCATACGGGGCCTAGTCCTTACATGGCGATGAAG\n","result = predict_mutation(model, tokenizer, label_encoder, sequence, device)\n","\n","print(\"\\nPrediction Result:\")\n","print(f\"Predicted Label: {result['predicted_label']}\")\n","print(\"\\nClass Probabilities:\")\n","for label, prob in result['probabilities'].items():\n","        print(f\"{label}: {prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RAoTa_MZkUhX","executionInfo":{"status":"ok","timestamp":1732391704688,"user_tz":-60,"elapsed":948,"user":{"displayName":"OUAFAE ELFAIDI","userId":"10715894589759469388"}},"outputId":"b790038b-5b88-43a6-8559-4cc1429a1cff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Prediction Result:\n","Predicted Label: Intermediate\n","\n","Class Probabilities:\n","Intermediate: 0.9988\n","Normal: 0.0004\n","Reduced_Penetrance: 0.0008\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"MnkZk0PDcWXE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"K_FL3jos_O2G"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":734},"id":"h_NImLgMNKC9","outputId":"07d2c2ba-63ed-4585-f87d-a763462413d5","executionInfo":{"status":"error","timestamp":1735491166835,"user_tz":-60,"elapsed":3969483,"user":{"displayName":"OUAFAE ELFAIDI","userId":"10715894589759469388"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNA_bert_6 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch 1/10: 100%|██████████| 250/250 [05:56<00:00,  1.42s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Average Loss = 0.3965, Accuracy = 93.30%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10: 100%|██████████| 250/250 [06:00<00:00,  1.44s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Average Loss = 0.1991, Accuracy = 93.90%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10: 100%|██████████| 250/250 [06:00<00:00,  1.44s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: Average Loss = 0.1684, Accuracy = 88.20%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10: 100%|██████████| 250/250 [05:59<00:00,  1.44s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Average Loss = 0.1495, Accuracy = 95.00%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10: 100%|██████████| 250/250 [06:00<00:00,  1.44s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: Average Loss = 0.1228, Accuracy = 94.50%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10: 100%|██████████| 250/250 [06:00<00:00,  1.44s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6: Average Loss = 0.1117, Accuracy = 94.60%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/10: 100%|██████████| 250/250 [06:00<00:00,  1.44s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7: Average Loss = 0.0785, Accuracy = 94.00%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/10: 100%|██████████| 250/250 [06:00<00:00,  1.44s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8: Average Loss = 0.0612, Accuracy = 94.70%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/10: 100%|██████████| 250/250 [06:00<00:00,  1.44s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9: Average Loss = 0.0441, Accuracy = 94.80%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/10: 100%|██████████| 250/250 [06:00<00:00,  1.44s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10: Average Loss = 0.0381, Accuracy = 95.70%\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"Cannot find file: /content/drive/MyDrive/our_project/model/dnabert_state_dict.pt","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-d475a71247bc>\u001b[0m in \u001b[0;36m<cell line: 179>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/our_project/model/dnabert_state_dict.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/our_project/model/labelencoder.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/our_project/model/labelencoder.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: /content/drive/MyDrive/our_project/model/dnabert_state_dict.pt"]}],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import os\n","import json\n","\n","class DNASequenceDataset(Dataset):\n","    def __init__(self, sequences, labels, tokenizer, max_length=512):\n","        self.sequences = sequences\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.sequences)\n","\n","    def __getitem__(self, idx):\n","        sequence = self.sequences[idx]\n","        label = self.labels[idx]\n","\n","        # Convert sequence to k-mers (k=6 for DNABERT)\n","        kmers = [sequence[i:i+6] for i in range(len(sequence)-5)]\n","        kmer_sequence = \" \".join(kmers)\n","\n","        encoding = self.tokenizer(\n","            kmer_sequence,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","def load_dna_sequences(file_path):\n","    df = pd.read_csv(file_path)\n","    sequences = df.iloc[:, 0].values  # First column contains sequences\n","    labels = df.iloc[:, -1].values    # Last column contains labels\n","    return sequences, labels\n","\n","def train_dnabert_model(file_path, model_save_path, num_epochs=10):\n","      # Set device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Load and prepare data\n","    sequences, labels = load_dna_sequences(file_path)\n","\n","    # Encode labels\n","    label_encoder = LabelEncoder()\n","    encoded_labels = label_encoder.fit_transform(labels)\n","\n","    # Split data\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        sequences,\n","        encoded_labels,\n","        test_size=0.2,\n","        random_state=42\n","    )\n","\n","    # Load DNABERT tokenizer and model\n","    tokenizer = BertTokenizer.from_pretrained('zhihan1996/DNA_bert_6')\n","    model = BertForSequenceClassification.from_pretrained(\n","        'zhihan1996/DNA_bert_6',\n","        num_labels=len(label_encoder.classes_)\n","    ).to(device)\n","\n","    # Create datasets\n","    train_dataset = DNASequenceDataset(X_train, y_train, tokenizer)\n","    test_dataset = DNASequenceDataset(X_test, y_test, tokenizer)\n","\n","    # Create dataloaders\n","    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=16)\n","\n","    # Initialize optimizer\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n","\n","    # Training loop\n","    best_accuracy = 0\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0\n","\n","        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n","            optimizer.zero_grad()\n","\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                labels=labels\n","            )\n","\n","            loss = outputs.loss\n","            total_loss += loss.item()\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","        # Evaluation\n","        model.eval()\n","        correct = 0\n","        total = 0\n","\n","        with torch.no_grad():\n","            for batch in test_loader:\n","                input_ids = batch['input_ids'].to(device)\n","                attention_mask = batch['attention_mask'].to(device)\n","                labels = batch['labels'].to(device)\n","\n","                outputs = model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask\n","                )\n","\n","                _, predicted = torch.max(outputs.logits, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        accuracy = 100 * correct / total\n","        print(f'Epoch {epoch+1}: Average Loss = {total_loss/len(train_loader):.4f}, Accuracy = {accuracy:.2f}%')\n","\n","        if accuracy > best_accuracy:\n","            best_accuracy = accuracy\n","           # Create directories if they don't exist\n","\n","    return model, tokenizer, label_encoder\n","# Save model and encoder\n","\n","def predict_mutation(model, tokenizer, label_encoder, sequence, device):\n","    # Convert sequence to k-mers\n","    kmers = [sequence[i:i+6] for i in range(len(sequence)-5)]\n","    kmer_sequence = \" \".join(kmers)\n","\n","    # Tokenize\n","    encoding = tokenizer(\n","        kmer_sequence,\n","        add_special_tokens=True,\n","        max_length=512,\n","        padding='max_length',\n","        truncation=True,\n","        return_tensors='pt'\n","    )\n","\n","    # Move to device\n","    input_ids = encoding['input_ids'].to(device)\n","    attention_mask = encoding['attention_mask'].to(device)\n","\n","    # Get prediction\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n","        predicted_class = torch.argmax(probabilities, dim=1)\n","\n","    # Convert prediction to label\n","    predicted_label = label_encoder.inverse_transform(predicted_class.cpu().numpy())[0]\n","    probabilities = probabilities.cpu().numpy()[0]\n","\n","    return {\n","        'predicted_label': predicted_label,\n","        'probabilities': dict(zip(label_encoder.classes_, probabilities))\n","    }\n","\n","# Usage example:\n","if __name__ == \"__main__\":\n","    file_path = '/content/drive/MyDrive/our_project/dataset/labeled_DATA.csv'\n","    model_save_path = '/content/drive/MyDrive/our_project'\n","    encoder_save_path = '/content/drive/MyDrive/our_project/encoder'\n","\n","    # Train model\n","    model, tokenizer, label_encoder = train_dnabert_model(file_path, model_save_path)\n","    os.makedirs('/content/drive/MyDrive/our_project/model', exist_ok=True)\n","    torch.save(model,'/content/drive/MyDrive/our_project/model/update2.pt')\n","    torch.save(label_encoder, '/content/drive/MyDrive/our_project/model/labelencoder2.pt')\n","    torch.save(tokenizer, '/content/drive/MyDrive/our_project/model/tokenizer2.pt')\n","    num_labels = len(label_encoder.classes_)\n","    from google.colab import files\n","    files.download('/content/drive/MyDrive/our_project/model/update2.pt')\n","    files.download('/content/drive/MyDrive/our_project/model/labelencoder2.pt')\n","    files.download('/content/drive/MyDrive/our_project/model/tokenizer2.pt')\n","    with open('/content/drive/MyDrive/our_project/model/config.json', 'w') as f:\n","            json.dump({'num_labels': num_labels}, f)\n","        # Download the files"]},{"cell_type":"code","source":["!pip install Flask flask-ngrok\n","\n"],"metadata":{"id":"YgfWWmXYd-bS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733503060967,"user_tz":-60,"elapsed":2903,"user":{"displayName":"OUAFAE ELFAIDI","userId":"10715894589759469388"}},"outputId":"ac3fc9ed-281d-4470-81cc-e4bf0e7a9b56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Flask in /usr/local/lib/python3.10/dist-packages (3.0.3)\n","Collecting flask-ngrok\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.1.3)\n","Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.1.4)\n","Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from Flask) (8.1.7)\n","Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from Flask) (1.9.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.32.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2024.8.30)\n","Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Installing collected packages: flask-ngrok\n","Successfully installed flask-ngrok-0.0.25\n"]}]},{"cell_type":"code","source":["import torch\n","import gradio as gr\n","from transformers import BertTokenizer\n","import os\n","\n","def load_trained_model(model_path='/content/drive/MyDrive/our_project/dnabert_model.pt', encoder_path='/content/drive/MyDrive/our_project/label_encoder.pkl'):\n","    \"\"\"Load the trained DNA-BERT model and label encoder\"\"\"\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Load the model\n","    model = BertForSequenceClassification.from_pretrained(\n","        'zhihan1996/DNA_bert_6',\n","        num_labels=2  # Update this based on your number of classes\n","    )\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","    model.to(device)\n","    model.eval()\n","\n","    # Load the label encoder\n","    label_encoder = torch.load(encoder_path)\n","\n","    # Load the tokenizer\n","    tokenizer = BertTokenizer.from_pretrained('zhihan1996/DNA_bert_6')\n","\n","    return model, tokenizer, label_encoder, device\n","\n","def create_gradio_interface():\n","    \"\"\"Create and launch the Gradio interface\"\"\"\n","\n","    # First install gradio if not already installed\n","    !pip install -q gradio\n","\n","    # Load the model and components\n","    try:\n","        model, tokenizer, label_encoder, device = load_trained_model()\n","    except Exception as e:\n","        print(f\"Error loading model: {e}\")\n","        return\n","\n","    def predict_sequence(dna_sequence):\n","        \"\"\"Make prediction for a single DNA sequence\"\"\"\n","        try:\n","            # Convert sequence to k-mers\n","            kmers = [dna_sequence[i:i+6] for i in range(len(dna_sequence)-5)]\n","            kmer_sequence = \" \".join(kmers)\n","\n","            # Tokenize\n","            encoding = tokenizer(\n","                kmer_sequence,\n","                add_special_tokens=True,\n","                max_length=512,\n","                padding='max_length',\n","                truncation=True,\n","                return_tensors='pt'\n","            )\n","\n","            # Move to device\n","            input_ids = encoding['input_ids'].to(device)\n","            attention_mask = encoding['attention_mask'].to(device)\n","\n","            # Get prediction\n","            with torch.no_grad():\n","                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","                probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n","                predicted_class = torch.argmax(probabilities, dim=1)\n","\n","            # Convert prediction to label\n","            predicted_label = label_encoder.inverse_transform(predicted_class.cpu().numpy())[0]\n","            probs = probabilities.cpu().numpy()[0]\n","\n","            # Format the output\n","            result = f\"Predicted Class: {predicted_label}\\n\\nProbabilities:\\n\"\n","            for label, prob in zip(label_encoder.classes_, probs):\n","                result += f\"{label}: {prob:.4f}\\n\"\n","\n","            return result\n","\n","        except Exception as e:\n","            return f\"Error processing sequence: {str(e)}\"\n","\n","    # Create the interface\n","    interface = gr.Interface(\n","        fn=predict_sequence,\n","        inputs=gr.Textbox(\n","            lines=3,\n","            placeholder=\"Enter DNA sequence here...\",\n","            label=\"DNA Sequence\"\n","        ),\n","        outputs=gr.Textbox(label=\"Prediction Results\"),\n","        title=\"Huntington's Disease Mutation Predictor\",\n","        description=\"Enter a DNA sequence to predict the likelihood of Huntington's disease mutation.\",\n","        examples=[\n","            [\"ATGGCGACCCTGGAAAAGCTGATGAAGGCCTTCGAGTCCCTCAAGTCCTTC\"],  # Add some example sequences\n","        ]\n","    )\n","\n","    return interface\n","\n","def deploy_model():\n","    \"\"\"Main deployment function\"\"\"\n","    print(\"Starting DNA-BERT model deployment...\")\n","\n","    # Create and launch the interface\n","    interface = create_gradio_interface()\n","    if interface:\n","        interface.launch(share=True)\n","        print(\"\\nModel deployed successfully! Use the URL above to access the interface.\")\n","    else:\n","        print(\"\\nDeployment failed. Please check the error messages above.\")"],"metadata":{"id":"8QZAs9-geeqB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["   deploy_model()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u2jG5DQqB_uy","executionInfo":{"status":"ok","timestamp":1735483846853,"user_tz":-60,"elapsed":6618,"user":{"displayName":"OUAFAE ELFAIDI","userId":"10715894589759469388"}},"outputId":"7b17d67f-85b7-452b-edb1-c759d2191326"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting DNA-BERT model deployment...\n","Error loading model: name 'BertForSequenceClassification' is not defined\n","\n","Deployment failed. Please check the error messages above.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"xqrJRDUkp0vD","colab":{"base_uri":"https://localhost:8080/","height":897},"executionInfo":{"status":"error","timestamp":1733506405751,"user_tz":-60,"elapsed":16823,"user":{"displayName":"OUAFAE ELFAIDI","userId":"10715894589759469388"}},"outputId":"2f28b112-47a5-47c1-d718-29c55c5c2939"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["2024-12-06 17:33:12.254 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-06 17:33:12.441 \n","  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n","  command:\n","\n","    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n","2024-12-06 17:33:12.453 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-06 17:33:12.455 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-06 17:33:12.458 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-06 17:33:12.462 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-06 17:33:12.464 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-06 17:33:12.467 Session state does not function when running a script without `streamlit run`\n","2024-12-06 17:33:12.469 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-06 17:33:12.472 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-06 17:33:12.475 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-06 17:33:12.477 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-06 17:33:12.480 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-06 17:33:12.482 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-06 17:33:12.485 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n","added 22 packages in 4s\n","\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n","\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K3 packages are looking for funding\n","\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K  run `npm fund` for details\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:pyngrok.process.ngrok:t=2024-12-06T17:33:26+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n","ERROR:pyngrok.process.ngrok:t=2024-12-06T17:33:26+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n","ERROR:pyngrok.process.ngrok:t=2024-12-06T17:33:26+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n","CRITICAL:pyngrok.process.ngrok:t=2024-12-06T17:33:26+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"]},{"output_type":"error","ename":"PyngrokNgrokError","evalue":"The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-abe387033e69>\u001b[0m in \u001b[0;36m<cell line: 110>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# Create a public URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8501\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Public URL: {public_url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"auth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    399\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                                     ngrok_process.startup_error)\n","\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n."]}]}]}